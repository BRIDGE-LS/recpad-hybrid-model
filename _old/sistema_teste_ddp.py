#!/usr/bin/env python3
"""
TESTE R√ÅPIDO DDP - Detecta problemas em 5-10 minutos
=====================================================

Execute este script ANTES do treinamento completo para identificar
se haver√° problemas de timeout NCCL.
"""

import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
import time
import datetime
from pathlib import Path

def test_ddp_basic(rank, world_size):
    """Teste b√°sico de DDP com opera√ß√µes que causam timeout"""
    try:
        print(f"[Rank {rank}] Iniciando teste...")
        
        # Configurar ambiente
        os.environ['MASTER_ADDR'] = '127.0.0.1'
        os.environ['MASTER_PORT'] = '12355'
        
        # TESTE 1: Init process group (pode travar aqui)
        print(f"[Rank {rank}] Teste 1: Inicializando process group...")
        start_time = time.time()
        
        dist.init_process_group(
            backend='nccl',
            rank=rank,
            world_size=world_size,
            timeout=datetime.timedelta(minutes=2)  # Timeout curto para teste
        )
        
        init_time = time.time() - start_time
        print(f"[Rank {rank}] ‚úÖ Init OK em {init_time:.2f}s")
        
        # Configurar device
        torch.cuda.set_device(rank)
        device = torch.device(f'cuda:{rank}')
        
        # TESTE 2: Opera√ß√µes coletivas b√°sicas
        print(f"[Rank {rank}] Teste 2: Opera√ß√µes coletivas...")
        
        # AllReduce
        tensor = torch.randn(1000, 1000, device=device)
        dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
        print(f"[Rank {rank}] ‚úÖ AllReduce OK")
        
        # AllGather (opera√ß√£o que falhou no erro original)
        tensor_list = [torch.zeros_like(tensor) for _ in range(world_size)]
        dist.all_gather(tensor_list, tensor)
        print(f"[Rank {rank}] ‚úÖ AllGather OK")
        
        # TESTE 3: Broadcast
        if rank == 0:
            broadcast_tensor = torch.randn(2000, 2000, device=device)
        else:
            broadcast_tensor = torch.zeros(2000, 2000, device=device)
        
        dist.broadcast(broadcast_tensor, src=0)
        print(f"[Rank {rank}] ‚úÖ Broadcast OK")
        
        # TESTE 4: Barrier (pode travar)
        print(f"[Rank {rank}] Teste 4: Barrier...")
        dist.barrier()
        print(f"[Rank {rank}] ‚úÖ Barrier OK")
        
        # TESTE 5: Simular carga de trabalho desbalanceada
        print(f"[Rank {rank}] Teste 5: Carga desbalanceada...")
        if rank == 0:
            time.sleep(2)  # Rank 0 demora mais
        
        dist.barrier()  # Todos esperam
        print(f"[Rank {rank}] ‚úÖ Carga desbalanceada OK")
        
        print(f"[Rank {rank}] üéâ TODOS OS TESTES PASSARAM!")
        return True
        
    except Exception as e:
        print(f"[Rank {rank}] ‚ùå ERRO: {e}")
        return False
    finally:
        try:
            dist.destroy_process_group()
        except:
            pass

def test_nccl_availability():
    """Testa se NCCL est√° funcionando corretamente"""
    print("üîç Testando disponibilidade NCCL...")
    
    if not torch.cuda.is_available():
        print("‚ùå CUDA n√£o dispon√≠vel")
        return False
    
    gpu_count = torch.cuda.device_count()
    print(f"üìä GPUs detectadas: {gpu_count}")
    
    if gpu_count < 2:
        print("‚ö†Ô∏è  Menos de 2 GPUs - DDP n√£o ser√° usado")
        return False
    
    # Testar NCCL backend
    try:
        if dist.is_nccl_available():
            print("‚úÖ NCCL dispon√≠vel")
        else:
            print("‚ùå NCCL n√£o dispon√≠vel")
            return False
    except:
        print("‚ùå Erro ao verificar NCCL")
        return False
    
    return True

def run_quick_ddp_test():
    """Executa teste r√°pido completo"""
    print("="*60)
    print("üß™ TESTE R√ÅPIDO DDP - DETEC√á√ÉO DE PROBLEMAS")
    print("="*60)
    
    # Teste 1: Verifica√ß√µes b√°sicas
    if not test_nccl_availability():
        print("\n‚ùå Falha nos testes b√°sicos - use single GPU")
        return False
    
    # Teste 2: DDP real
    world_size = min(torch.cuda.device_count(), 2)  # M√°ximo 2 GPUs para teste
    
    print(f"\nüöÄ Testando DDP com {world_size} GPUs...")
    print("‚è±Ô∏è  Isso deve levar 30-60 segundos...")
    
    try:
        # Configura√ß√µes NCCL para teste
        os.environ['NCCL_BLOCKING_WAIT'] = '1'
        os.environ['NCCL_ASYNC_ERROR_HANDLING'] = '1'
        os.environ['NCCL_DEBUG'] = 'WARN'
        
        mp.spawn(test_ddp_basic, args=(world_size,), nprocs=world_size, join=True)
        
        print("\nüéâ TESTE DDP PASSOU - Sistema pronto para treinamento distribu√≠do!")
        return True
        
    except Exception as e:
        print(f"\n‚ùå TESTE DDP FALHOU: {e}")
        print("\nüí° RECOMENDA√á√ÉO: Use single GPU (use_distributed=False)")
        return False

if __name__ == "__main__":
    success = run_quick_ddp_test()
    
    if success:
        print("\n" + "="*60)
        print("‚úÖ RESULTADO: Pode usar DDP com seguran√ßa")
        print("üìù Configure: use_distributed=True")
        print("="*60)
    else:
        print("\n" + "="*60)
        print("‚ö†Ô∏è  RESULTADO: Use single GPU")
        print("üìù Configure: use_distributed=False")
        print("="*60)